{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ceaec7b-be82-4482-bb48-6369646895f5",
   "metadata": {},
   "source": [
    "[![Abrir en Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adan-rs/AnalisisDatos/blob/main/notebooks/05_Preparar_datos.ipynb)\n",
    "\n",
    "# Preparación de datos\n",
    "*\"Es un error capital teorizar antes de tener datos. Sin darse cuenta, uno empieza a deformar los hechos para que se ajusten a las teorías, en lugar de ajustar las teorías a los hechos.\" (Arthur Conan Doyle en \"Escándalo en Bohemia\")*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a14e52-8f4f-4ffc-adb4-ff5ad9226284",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "El análisis de datos comienza mucho antes de aplicar modelos estadísticos o construir visualizaciones. Dos etapas clave que determinan la calidad del análisis son la preparación de datos y el procesamiento de datos.\n",
    "\n",
    "La preparación de datos es una etapa inicial con enfoque manual y exploratorio. Aquí el objetivo es comprender la estructura y calidad de los datos: se revisan los tipos de variables, se detectan valores faltantes, se identifican errores o inconsistencias, y se decide qué información conservar. Esta etapa implica el juicio humano y suele apoyarse en comandos como .head(), .info(), .describe(), así como inspección visual directa del contenido.\n",
    "\n",
    "Una vez comprendidos los datos, pasamos al procesamiento de datos, que implica un enfoque automatizado y reproducible. En esta etapa se aplican transformaciones programadas que pueden incluir la eliminación de duplicados, tratamiento de valores perdidos y tratamiento de valores atípicos. Estas tareas se sistematizan mediante funciones, scripts o pipelines, lo que permite aplicar los mismos pasos a nuevos conjuntos de datos sin intervención manual.\n",
    "\n",
    "Entender esta distinción ayuda a formar buenos hábitos: primero explorar con criterio, luego automatizar con claridad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf34ac-cf04-4e91-ab7c-3f25ac0238f0",
   "metadata": {},
   "source": [
    "## Descripción de datos\n",
    "Airbnb es una plataforma en línea que permite a los usuarios alquilar alojamientos. El archivo \"listings_cdmx.csv\" contiene el listado de alojamientos en la Ciudad de México, con información básica actualizada al 25 de septiembre de 2024. \n",
    "\n",
    "Las variables disponibles son: \n",
    "- id: identificador del anuncio de alojamiento.\n",
    "- name: nombre del alojamiento.\n",
    "- host_id: identificador del anfitrión.\n",
    "- host_name: nombre del anfitrión.\n",
    "- neighbourhood_group: grupo.\n",
    "- neighbourhood: alcaldía.\n",
    "- latitude: latitud.\n",
    "- longitude: longitud.\n",
    "- room_type: puede ser \"Entire place\", \"Private room\", \"Shared room\" u \"Hotel\".\n",
    "- price: precio en moneda nacional.\n",
    "- minimum_nights: mínimo de noches de estadía.\n",
    "- last_review: fecha de la última reseña.\n",
    "- reviews_per_month: promedio de reseñas por mes en el tiempo publicado.\n",
    "- calculated_host_listings_count: número de anuncios que tiene el anfitrión.\n",
    "- availability_365: días disponibles en los siguientes 365 días.\n",
    "- number_of_reviews_ltm: número de reseñas en los últimos 12 meses.\n",
    "- license: número de licencia o registro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e650e26a-cef4-4351-b418-66da8ac12e66",
   "metadata": {},
   "source": [
    "## Carga y selección de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147a8a4a-ad36-479a-968b-ef3a6b8dcff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27de0c0f-b51e-43dc-bd8d-16503882f982",
   "metadata": {},
   "source": [
    "**Usa de nombres descriptivos para cada DataFrame**: Aunque es común usar `df` como nombre de un DataFrame, es recomendable emplear nombres descriptivos que indiquen si los datos son originales (*df_original, raw_data, input_data, source_df*), seleccionados o procesados. Esto facilitará la comprensión del código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9109c73-2b62-4e45-b2b9-5ccc4e131a7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Carga de archivo\n",
    "url = 'https://github.com/adan-rs/AnalisisDatos/raw/main/data/listings_cdmx.csv'\n",
    "df_original = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7954fbe1-6576-4a89-bd6e-1b81ec3efd02",
   "metadata": {},
   "source": [
    "**Selecciona las columnas**: Si de antemano ya sabes con qué columnas vas a trabajar, puedes utilizar el parámetro `usecols` directamente en la función `pd.read_csv` o `pd.read_excel`. Esto tiene la ventaja de optimizar el espacio en memoria que ocupa el dataframe. Por ejemplo:\n",
    "```\n",
    "url = 'https://github.com/adan-rs/AnalisisDatos/raw/main/data/listings_cdmx.csv'\n",
    "columnas = [\"host_id\", \"neighbourhood\", \"room_type\", \"price\", \n",
    "             \"minimum_nights\", \"number_of_reviews_ltm\", \"license\"]\n",
    "df_original = pd.read_csv(ruta, usecols=columnas)\n",
    "```\n",
    "Sin embargo, en un análisis exploratorio es preferible cargar los datos completos y luego seleccionar las columnas como haremos a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409cae53-5124-4b12-800b-fe370230b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de columnas\n",
    "columnas = [\"host_id\", \"neighbourhood\", \"room_type\", \"price\", \n",
    "            \"minimum_nights\", \"number_of_reviews_ltm\", \"license\"]\n",
    "df_columnas = df_original[columnas]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a29d0c-b29a-4dbb-9fa2-61ab9a0f1b93",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Selección de filas**: Para seleccionar filas bajo condiciones específicas, usa expresiones lógicas. Supongamos que sólo nos interesa analizar casas/departamentos que estén hayan tenido una reseña en el último año. Podemos filtrar entonces:\n",
    "- Anuncios de casas o departamentos:  \n",
    "  `df_columnas['room_type'] == 'Entire home/apt'`\n",
    "- Anuncios con al menos una reseña en el último año:  \n",
    "  `df_columnas['number_of_reviews_ltm'] > 0`\n",
    "\n",
    "Combinaremos ambas condiciones en un filtro colocando cada condición dentro de un paréntesis y luego aplicamos ese filtro con `df_filtrado = df_filtrado[filtro]` o con `df_filtrado = df_filtrado.loc[filtro]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a5a2a9-7b34-4fd9-b0fc-1083a00b0191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de filas (a partir del dataframe con columnas seleccionadas)\n",
    "filtro = (df_columnas['room_type'] == 'Entire home/apt') & (df_columnas['number_of_reviews_ltm'] > 0)\n",
    "df_filtrado = df_columnas[filtro]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa44dbaa-6656-4309-a7e3-a915fcdc849e",
   "metadata": {},
   "source": [
    "Es recomendable utilizar `.copy()` para trabajar con una copia independiente y no una vista ligada al original. Esto es importante porque con esto se evita el mensaje `SettingWithCopyWarning` al hacer modificaciones a este dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0354dda-933d-474d-8ef9-537615d1ad6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Crear copia del DataFrame original\n",
    "df = df_filtrado.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407352fb-391c-484c-a06f-5c4f7d4d3ecf",
   "metadata": {},
   "source": [
    "**Encapsulación del proceso (opcional)**: Para mantener un código limpio y reutilizable, encapsula los pasos anteriores en una función personalizada con un nombre descriptivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70fca65-6fb6-42b4-a390-4bf8ea37dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter(filepath):\n",
    "    \"\"\"\n",
    "    Carga y filtrado de datos originales.\n",
    "    \"\"\"\n",
    "    columns = [\"host_id\", \"neighbourhood\", \"room_type\", \"price\", \n",
    "               \"minimum_nights\", \"number_of_reviews_ltm\", \"license\"]\n",
    "    data = pd.read_csv(filepath, usecols=columns)\n",
    "    filtro = (data['room_type'] == 'Entire home/apt') & (data['number_of_reviews_ltm'] > 0)\n",
    "    return data.loc[filtro].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea2388-e67d-45f9-ae8b-7a2c02062e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/adan-rs/AnalisisDatos/raw/main/data/listings_cdmx.csv'\n",
    "df = load_and_filter(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6bafae-3695-412d-87fe-3d80a133bc4f",
   "metadata": {},
   "source": [
    "## Revisión general de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1970af86-4d8e-4f81-a807-e47bfcccb45d",
   "metadata": {},
   "source": [
    "Revisa la información del DataFrame con los siguientes comandos:\n",
    "- `df.columns`: muestra los nombres de las columnas \n",
    "- `df.shape`: muestra el número de filas y columnas.\n",
    "- `df.info()`: proporciona un resumen de las columnas, incluyendo sus tipos de datos y valores nulos.\n",
    "- `df.head()`: muestra las primeras filas del DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e22e9a-3f43-4fce-bedd-ae253c184bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de filas y columnas\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d9bbde-c489-4a78-ad90-aa6ac2381bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7616b2-5ad8-4402-831f-051e56766c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caff1fb5-0445-4655-80f2-a041550e76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipos de datos\n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369cb726-9d65-410f-b97c-bcaad60fdf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores nulos por variable\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91295a88-9c43-4bab-98ed-70831a9de60d",
   "metadata": {},
   "source": [
    "*Estadística descriptiva: medidas numéricas (opcional)*: Si deseas obtener las principales medidas numéricas de las variables cualitativas utiliza `df.describe()`.\n",
    "\n",
    "*Estadística descriptiva: variables categóricas (opcional)*: Si deseas conocer el número de valores únicos en cada variable categórica puedes utilizar `df[col].nunique()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5565d6-896f-4f26-8b36-713026fcd5eb",
   "metadata": {},
   "source": [
    "## Análisis rápido\n",
    "\n",
    "Construiremos una función para el análisis rápido de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2be4122-f3c6-49ec-8ebb-428fa053df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_analysis(df):\n",
    "    \"\"\"Análisis rápido del DataFrame\"\"\"\n",
    "    print(\"\\nANÁLISIS RÁPIDO\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Dimensiones del DataFrame: {df.shape}\")\n",
    "    print(f\"Tipos de datos:\\n{df.dtypes}\")\n",
    "    print(f\"\\nValores faltantes:\")\n",
    "    missing = df.isnull().sum()\n",
    "    print(missing[missing > 0])\n",
    "    \n",
    "    print(f\"\\nMedidas numéricas:\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(df[numeric_cols].describe().round(2))\n",
    "    \n",
    "    print(f\"\\nVariables categóricas:\")\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in cat_cols:\n",
    "        print(f\"{col}: {df[col].nunique()} valores únicos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d915f-31e8-4051-a20e-fcf7a4fceae1",
   "metadata": {},
   "source": [
    "En el código anterior se utilizó una *f-string*. Una f-string (formatted string literal) es una forma concisa e intuitiva de incluir variables o expresiones dentro de un texto en Python. Se introdujo en Python 3.6 y su sintaxis es `f\"Texto con una variable: {variable}\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129145aa-18aa-4b5d-98d5-5a5c522f7c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_analysis(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff76f7a6-e742-4831-82cf-fcc3026de17e",
   "metadata": {},
   "source": [
    "**Detección de inconsistencias en tipos de variables:** El análisis de la información anterior nos revela que, aunque la columna host_id contiene valores numéricos  no representa una variable cuantitativa. Debemos reclasificarla para evitar operaciones matemáticas inapropiadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9492e4-af83-4456-981a-6564ae02e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"host_id\"] = df[\"host_id\"].astype('object')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f6e438-ef7e-4445-94b0-d76db0383dac",
   "metadata": {},
   "source": [
    "# Procesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e121b10a-7c83-4249-bfe7-8b87bd3bb70c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Datos duplicados\n",
    "**Identificar datos duplicados**: En ocasiones, nuestra base de datos puede contener información duplicada. Para identificar estos datos duplicados, podemos utilizar el método duplicated() de las siguientes maneras:\n",
    "- Para visualizar todas las filas duplicadas:  \n",
    "  `df[df.duplicated()]`\n",
    "- Si nos interesa analizar duplicados en una columna específica (por ejemplo, \"A\"):  \n",
    "  `df[df['A'].duplicated()]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5228a140-5d4b-4a63-8b23-2a37b446f208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Conteo de duplicados en todas las variables \n",
    "numero_duplicados = df.duplicated().sum()\n",
    "print(f\"Filas duplicadas: {numero_duplicados}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332e92d6-d878-41e0-b8dd-4920f6c1be90",
   "metadata": {},
   "source": [
    "**Eliminar datos duplicados**: Para eliminar datos duplicados, se utiliza el método `drop_duplicates()` que conserva la primera ocurrencia de cada duplicado. En el ejemplo general, podemos eliminar filas duplicadas (considerando todas las variables) mediante:  \n",
    "`df = df.drop_duplicates()`  \n",
    "Si necesitamos eliminar duplicados considerando sólo algunas variables, podemos usar el parámetro subset:  \n",
    "`df = df.drop_duplicates(subset=['hotel_id'])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027720a8-b877-418b-bdae-857119dcb154",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23ad92-68c6-452a-a868-418cf8fbb0e2",
   "metadata": {},
   "source": [
    "## Datos perdidos\n",
    "Los datos perdidos pueden influir significativamente en los resultados del análisis, por lo que es importante determinar si estos son aleatorios o si siguen algún patrón específico. Por ejemplo, en una encuesta, es posible que quienes omitan informar su ingreso compartan ciertas características o que preguntas sobre comportamientos socialmente indeseables tengan tasas de no respuesta más altas.\n",
    "\n",
    "Para determinar si los datos perdidos son aleatorios, es común realizar pruebas estadísticas. El procedimiento general incluye:\n",
    "- Dividir las observaciones en dos grupos:\n",
    "    - Grupo con datos completos.\n",
    "    - Grupo con valores perdidos en una variable específica.\n",
    "- Comparar los valores promedio de otras variables entre ambos grupos mediante pruebas de significancia.\n",
    "\n",
    "Estas pruebas permiten identificar si los datos perdidos podrían estar relacionados con alguna característica específica de los datos.\n",
    "\n",
    "**Análisis inicial de datos perdidos**: Una herramienta útil para identificar variables con valores perdidos es el método `info()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4c7073-a004-496d-8f5a-be7d878acb2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identifica qué variables tienen valores perdidos con 'info()'\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d587b488-94c7-494a-a270-fcd302d6ab01",
   "metadata": {},
   "source": [
    "Para identificar filas con al menos un valor perdido, podemos usar `df[df.isna().any(axis=1)]`\n",
    "donde`isna()` identifica los valores perdidos y `any(axis=1)` verifica si existe al menos un valor perdido en cada fila."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df3e86-22a0-43c0-a84e-ff58d097443e",
   "metadata": {},
   "source": [
    "Para encontrar las filas con datos perdidos de una variable en particular (por ejemplo \"price\") podemos usar `df[df.price.isna()]`. Otra opción equivalente a `isna()` es `isnull()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc5d90-bb13-42d6-8183-f25619c3eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar filas con datos perdidos en una variable\n",
    "df[df.price.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7085062f-edda-4284-8de7-1e80293166ce",
   "metadata": {},
   "source": [
    "**¿Qué hacer con datos perdidos?**  \n",
    "Existen diversas estrategias para tratar los datos faltantes, dependiendo del contexto y las características del problema. A continuación, se presentan las más comunes:\n",
    "\n",
    "1. *Eliminar filas o columnas con datos faltantes*: Si una columna no es esencial y tiene una gran cantidad de datos faltantes (por ejemplo, más del 20%), puede ser razonable excluirla del análisis. Si solo unas pocas filas contienen datos faltantes (por ejemplo, menos del 5%) y estas ausencias parecen ser completamente aleatorias, eliminarlas puede ser una solución viable. Sin embargo, en el caso de series de tiempo, eliminar filas puede distorsionar los patrones temporales y no es una práctica recomendada (¿por qué crees que esto ocurre?).\n",
    "\n",
    "2. *Codificación*: Los valores faltantes también pueden contener información valiosa. En variables categóricas, se pueden codificar como una categoría adicional para incluirlos en el análisis.\n",
    "\n",
    "3. *Imputación simple*: Consiste en reemplazar los valores faltantes con un estimado calculado a partir de la misma variable. Una opción común y conservadora es usar el promedio o la mediana. Sin embargo, este método tiene limitaciones, ya que reduce la variabilidad y puede sesgar los intervalos de confianza (Treiman, 2009).\n",
    "\n",
    "4. *Imputación multivariada*: Este enfoque utiliza otras variables del conjunto de datos para estimar los valores faltantes. Por ejemplo, en la imputación basada en regresión, se construye una ecuación de regresión donde la variable dependiente es aquella con datos faltantes. Luego, los valores se predicen utilizando la ecuación estimada.\n",
    "\n",
    "Para profundizar en otros métodos de imputación, se recomienda la lectura del capítulo “Multiple Imputation of Missing Data” de Treiman (2009).\n",
    "\n",
    "*Imputación simple o univariada*  \n",
    "La imputación con la media para una columna 'A' se obtiene con:   \n",
    "`df['A'] = df['A'].fillna(df['A'].mean())`  \n",
    "En caso de querer la imputación con la mediana se reemplaza `mean()` con `median()` y en el caso de la moda se reemplaza con `mode().iloc[0]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9214cd65-7549-45f7-a9b6-4d29d933034d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_missing_data(df, threshold=0.2):\n",
    "    \"\"\" \n",
    "    Maneja datos perdidos según el tipo de datos de cada columna, \n",
    "    y elimina columnas con un porcentaje de datos perdidos mayor a \n",
    "    un umbral específico. \n",
    "    Retorna DataFrame procesado sin valores nulos.\n",
    "    \"\"\"  \n",
    "    print(\"\\nTRATAMIENTO DE VALORES PERDIDOS\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Antes: valores faltantes por columna:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Remove columns with high proportion of missing values\n",
    "    cols_with_nulls = df.isnull().mean() > threshold\n",
    "    cols_to_drop = df.columns[cols_with_nulls]\n",
    "    if len(cols_to_drop) > 0:\n",
    "        print(f\"\\nColumnas eliminadas por tener más de {threshold*100}% de valores nulos:\")\n",
    "        print(cols_to_drop.tolist())\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # Handle missing values in remaining columns usando loc (evita el warning)\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            # Para columnas numéricas: usar mediana\n",
    "            mask = df[col].isna()\n",
    "            df.loc[mask, col] = df[col].median()\n",
    "        elif df[col].dtype == 'object':\n",
    "            # Para columnas categóricas: usar 'DESCONOCIDO'\n",
    "            mask = df[col].isna()\n",
    "            df.loc[mask, col] = 'DESCONOCIDO'\n",
    "            \n",
    "    total_remaining_nulls = df.isna().sum().sum()\n",
    "    print(f\"\\nDespués: valores nulos restantes: {total_remaining_nulls}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2179032-ccb5-4a59-bf11-3d1371e9b2ba",
   "metadata": {},
   "source": [
    "En el código anterior aparece un *iterador* `for..in` que se usa para recorrer de manera secuencial los elementos de una colección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67add1f2-ea70-41d4-8e98-c34c6f0deb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = handle_missing_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7926b48-ed9d-484e-b007-6e17805ac3b0",
   "metadata": {},
   "source": [
    "**Nota**: Estas funciones están diseñadas para datos de corte transversal (cross-sectional) y no deben aplicarse directamente a series de tiempo. En series de tiempo hay que considerar los patrones que tenga la serie para hacer la estimación o interpolación apropiada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2febcc66-3fb4-404d-afa3-ac0826c58a31",
   "metadata": {},
   "source": [
    "## Valores atípicos\n",
    "Un valor atípico (outlier) es un dato extremo en una o más variables que se desvía significativamente del resto de las observaciones. \n",
    "\n",
    "**Identificación de datos atípicos**  \n",
    "- En el caso de series univariadas, es común utilizar el método del valor z o el criterio del rango intercuartil para identificar valores atípicos.\n",
    "- Para el caso de datos multivariados se utilizan técnicas estadísticas avanzadas o algoritmos como *isolation forest*.\n",
    "\n",
    "**¿Qué hacer con datos atípicos?**\n",
    "- Si el valor extremo es un error de captura o parte de otra población lo recomendable es corregir o borrar el caso o variable.\n",
    "- Si el valor extremo es parte de los datos que nos interesa analizar se debe mantener (p. ej. ventas en navidad).\n",
    "- En algunas variables económicas se recomienda transformar la variable de forma tal que el valor extremo no impacte los resultados (p. ej. transformación logarítmica, transformación de Box-Cox o la recodificación de datos).\n",
    "\n",
    "\n",
    "*Método del valor z*: En este enfoque un valor atípico es aquel que esté a más de tres desviaciones estándar a partir de la media. Es un enfoque sencillo y ampliamente utilizado en series univariadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d1042-ecea-49a2-a98c-21de5b1ab0e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_outliers_3s(df, column):\n",
    "    \"\"\"\n",
    "    Elimina valores atípicos utilizando el método del valor z. \n",
    "    Retorna un DataFrame sin valores atípicos.\n",
    "    \"\"\"\n",
    "    mean = df[column].mean()\n",
    "    std = df[column].std()\n",
    "    upper_limit = mean + 3 * std\n",
    "    lower_limit = mean - 3 * std\n",
    "    df_clean = df[(df[column] > lower_limit) & (df[column] < upper_limit)]\n",
    "    excluded_values = len(df) - len(df_clean)\n",
    "    print(f\"Cantidad de valores atípicos excluidos: {excluded_values}\")\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b971c0-8f65-4076-9dff-b62ee2056d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_outliers_3s(df,'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb30c45-72a1-4144-b998-22f0eb4fc829",
   "metadata": {},
   "source": [
    "*Método del rango intercuartil*. Otro criterio común es considerar como atípicos los valores que están a más de 1.5 veces el rango intercuartil hacia el extremo a partir del 1er. o 3er. cuartil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde6704f-a6e5-4373-a4e0-3f00a750887f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(df, column):\n",
    "    \"\"\"\n",
    "    Elimina valores atípicos utilizando el criterio del rango intercuartil.\n",
    "    Retorna DataFrame sin valores atípicos.\n",
    "    \"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_limit = Q1 - 1.5 * IQR\n",
    "    upper_limit = Q3 + 1.5 * IQR\n",
    "    df_clean = df[(df[column] > lower_limit) & (df[column] < upper_limit)]\n",
    "    excluded_values = len(df) - len(df_clean)\n",
    "    print(f\"Cantidad de valores atípicos excluidos: {excluded_values}\")\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6db7f53-c21d-40ff-a620-1a239f1fb27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_outliers_iqr(df, 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7797bc-2120-4d13-9dfa-4c8194ec6ac4",
   "metadata": {},
   "source": [
    "*Algoritmo isolation forest*: Es un algoritmo utilizado para detectar valores atípicos en grandes conjuntos de datos. Divide los datos de manera aleatoria y cuanto más rápido se aísla un punto mayor es la probabilidad de que sea un valor atípico (anomalía)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86646141-a595-45e6-94ba-e6ec103113a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def remove_outliers_iso_forest(df, columns, contamination=0.05, random_state=42):\n",
    "    \"\"\"\n",
    "    Elimina valores atípicos utilizando el algoritmo Isolation Forest.\n",
    "    Retorna DataFrame sin valores atípicos.\n",
    "    \"\"\"\n",
    "    print(\"\\nTRATAMIENTO DE DATOS ATÍPICOS\")\n",
    "    print(\"-\" * 50)\n",
    "    # Initialize and fit Isolation Forest model\n",
    "    iso_forest = IsolationForest(contamination=contamination, random_state=random_state)\n",
    "    iso_forest.fit(df[columns])\n",
    "    \n",
    "    # Predict labels: 1 (normal) or -1 (outlier)\n",
    "    labels = iso_forest.predict(df[columns])\n",
    "    \n",
    "    # Calculate and display number of excluded outliers\n",
    "    df_clean = df[labels == 1]\n",
    "    excluded_values = len(df) - len(df_clean)\n",
    "    print(f\"Después: valores atípicos excluidos: {excluded_values}\")\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39bb791-2af9-4dec-add2-e08c350e9b21",
   "metadata": {},
   "source": [
    "Seleccionaremos este último criterio para filtrar datos atípicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fabbaf-3c20-425b-82d0-139f34ae6255",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_outliers_iso_forest(df, columns=['price', 'minimum_nights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc62031-4f8b-4fc6-8b76-a92fa2769ed7",
   "metadata": {},
   "source": [
    "**Nota**: Estas funciones están diseñadas para datos de corte transversal (cross-sectional) y no deben aplicarse directamente a series de tiempo. En el caso de las series de tiempo, se recomienda utilizar una ventana deslizante (rolling window) y, en lugar de eliminar datos, optar por métodos de estimación o interpolación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fb6cac-b8a4-43f7-b922-4c1661a836cc",
   "metadata": {},
   "source": [
    "## Exportar datos (opcional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ff54bb-1ce6-46dc-9b3f-a8f8f959c05c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Un dataframe lo podemos guardar con `df.to_excel('archivo.xlsx', index=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105dc65f-650c-4c21-af8d-388e4f1c96fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporta el dataframe depurado con el nombre 'output'\n",
    "# df.to_excel('output.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae45e6-83ac-41d9-955a-7bb55f3e26e7",
   "metadata": {},
   "source": [
    "## Resumen del flujo de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9673b6-a08b-4b4b-bf29-5b959ecfbc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archivo de origen\n",
    "url = 'https://github.com/adan-rs/AnalisisDatos/raw/main/data/listings_cdmx.csv'\n",
    "\n",
    "# Selección de datos\n",
    "df = load_and_filter(url)\n",
    "\n",
    "# Correcciones\n",
    "df[\"host_id\"] = df[\"host_id\"].astype('object')\n",
    "    \n",
    "# Eliminar filas duplicadas\n",
    "df = df.drop_duplicates(df)\n",
    "    \n",
    "# Manejar datos perdidos\n",
    "df = handle_missing_data(df)\n",
    "\n",
    "# Eliminar valores atípicos\n",
    "df = remove_outliers_iso_forest(df, columns=['price', 'minimum_nights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b2c442-b5a1-4ea5-bbb4-27b5355db2fc",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "En un notebook en blanco, ordena el código con las bibliotecas importadas, las funciones creadas y el flujo de trabajo para procesar los datos de Airbnb en otra ciudad. Reemplaza el filepath/url por `'../data/listings_madrid.csv'` o el enlace (url) al archivo original por:  \n",
    "`'https://data.insideairbnb.com/spain/comunidad-de-madrid/madrid/2024-09-11/visualisations/listings.csv'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ccbc9-beaf-4191-92d3-05d76b585120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44d2bf36-d30f-4cb6-a9dc-967d97886a99",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "- Una discusión interesante sobre el tratamiento de datos se puede encontrar en: Treiman, D. J. (2009). *Quantitative data analysis. Doing social research to test ideas*. San Francisco, CA: Jossey-Bass.\n",
    "- La base de datos fue tomada de https://insideairbnb.com/get-the-data/ para fines no comerciales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
