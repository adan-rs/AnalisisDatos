{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9adf0c9-d9ec-4e47-b9ae-61aa6f1a59e6",
   "metadata": {},
   "source": [
    "Para evaluar modelos de clasificación se suele recurrir a una matriz de confusión. Una matriz de confusión compara las predicciones del modelo con los valores reales. Para un problema binario, la matriz incluye:\n",
    "- Verdaderos positivos (VP): casos positivos correctamente predichos.\n",
    "- Falsos positivos (FP): casos negativos incorrectamente clasificados como positivos.\n",
    "- Verdaderos negativos (VN): casos negativos correctamente predichos.\n",
    "- Falsos negativos (FN): casos positivos no detectados (clasificados como negativos).\n",
    "\n",
    "A partir de estos valores, se calculan las siguientes métricas:\n",
    "\n",
    "Exactitud (accuracy): Indica el porcentaje de predicciones correctas. Esta métrica indica qué tan bien clasifica el modelo en general.  \n",
    "*Exactitud=  (VP+VN)/(VP+VN+FP+FN)*\n",
    "\n",
    "Precisión (precision): Indica qué proporción de los casos pronosticados como positivos realmente lo son. Esta métrica es importante cuando el costo de falsos positivos es alto.   \n",
    "*Precisión=VP/(VP+FP)*\n",
    "\n",
    "Sensibilidad (recall): Indica qué proporción de los casos realmente positivos fueron correctamente identificados. Esta métrica es importante cuando no detectar un positivo tiene consecuencias serias.  \n",
    "*Sensibilidad=VP/(VP+FN)*\n",
    "\n",
    "F1-score: El F1-score es la media armónica entre la precisión y la sensibilidad. Se utiliza cuando es importante encontrar un equilibrio entre ambos, especialmente en contextos donde hay clases desbalanceadas o donde tanto los falsos positivos como los falsos negativos tienen consecuencias relevantes. Un valor alto indica un buen desempeño tanto en precisión como en sensibilidad.  \n",
    "*F1-score=2×(Precisión×Sensibilidad)/(Precisión+Sensibilidad)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c2421e-9c80-4b33-a980-db59091829fb",
   "metadata": {},
   "source": [
    "*Ejemplo*\n",
    "Supón que un modelo de regresión logística para predecir si un cliente comprará un producto arrojó estos valores:\n",
    "- Verdaderos positivos (VP): 40\n",
    "- Falsos positivos (FP): 10\n",
    "- Verdaderos negativos (VN): 30\n",
    "- Falsos negativos (FN): 20\n",
    "\n",
    "\n",
    "| |Pronosticado: Sí\t|Pronósticado: No|\n",
    "|---|---|---|\n",
    "|Real: **Sí (positivos)**\t|Verdaderos positivos = 40\t|Falsos negativos = 20|\n",
    "|Real: **No (negativos)**\t|Falsos positivos = 10\t|Verdaderos Negativos = 30|\n",
    "\n",
    "\n",
    "Entonces:\n",
    "- Precisión = 40 / (40 + 10) = 0.80\n",
    "- Sensibilidad = 40 / (40 + 20) = 0.67\n",
    "- F1-score = 2 × (0.80 × 0.67) / (0.80 + 0.67) ≈ 0.73\n",
    "\n",
    "Esto indica que el modelo es razonablemente bueno para identificar compradores sin producir muchas falsas alarmas, aunque puede mejorarse en la detección de todos los casos positivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9ae881-538e-47cd-a84b-cf892f7838cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
